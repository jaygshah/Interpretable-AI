# Interpretable-AI readings and resources
-- In Progress --

I will be updatting this more often as I read more papers and related readings along my research

* [Feature Visualization](https://distill.pub/2017/feature-visualization/)
* [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/)
* [Using ArtiÔ¨Åcial Intelligence to Augment Human Intelligence](https://distill.pub/2017/aia/)
* [Visualizing Representations: Deep Learning and Human Beings](http://colah.github.io/posts/2015-01-Visualizing-Representations/)
* [Differentiable Image Parameterizations](https://distill.pub/2018/differentiable-parameterizations/)
* [Activation Atlas](https://distill.pub/2019/activation-atlas/)
* [Why we should rigorously measure and forecast AI progress?](https://www.lesswrong.com/posts/axzPYvcmWr2TwvnLi/an-101-why-we-should-rigorously-measure-and-forecast-ai)
1. [How can Interpretability help Alignment?](https://www.lesswrong.com/posts/uRnprGSiLGXv35foX/how-can-interpretability-help-alignment)

# Books

* [Interpretable ML-A Guide for Making Black Box Models Explainable-Christoph Molnar](https://christophm.github.io/interpretable-ml-book/)

# Projects

* [IBM AI Explainability 360 (v0.2.0)](https://github.com/IBM/AIX360/)