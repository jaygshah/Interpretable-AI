# Interpretable-AI readings and resources
-- In Progress --

I will be updatting this more often as I read more papers and related readings along my research

# Readings and Articles

* [Feature Visualization](https://distill.pub/2017/feature-visualization/)
* [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/)
* [Using ArtiÔ¨Åcial Intelligence to Augment Human Intelligence](https://distill.pub/2017/aia/)
* [Visualizing Representations: Deep Learning and Human Beings](http://colah.github.io/posts/2015-01-Visualizing-Representations/)
* [Differentiable Image Parameterizations](https://distill.pub/2018/differentiable-parameterizations/)
* [Activation Atlas](https://distill.pub/2019/activation-atlas/)
* [Why we should rigorously measure and forecast AI progress?](https://www.lesswrong.com/posts/axzPYvcmWr2TwvnLi/an-101-why-we-should-rigorously-measure-and-forecast-ai)
	1. [How can Interpretability help Alignment?](https://www.lesswrong.com/posts/uRnprGSiLGXv35foX/how-can-interpretability-help-alignment)
* [On Model Explainability From LIME, SHAP, to Explainable Boosting](https://everdark.github.io/k9/notebooks/ml/model_explain/model_explain.nb.html)

# Books

* [Interpretable ML-A Guide for Making Black Box Models Explainable-Christoph Molnar](https://christophm.github.io/interpretable-ml-book/)

# Projects

* [IBM AI Explainability 360 (v0.2.0)](https://github.com/IBM/AIX360/)
* [Lime](https://github.com/marcotcr/lime)
* [SHAP](https://github.com/slundberg/shap)
* [InterpretML](https://github.com/interpretml/interpret)

---

# Cool Tools
* [Play with a Neural Network](https://github.com/jaygshah/Interpretable-AI.git)